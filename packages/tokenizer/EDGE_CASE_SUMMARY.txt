===============================================================================
RS-BPE EDGE CASE TESTING RESULTS
===============================================================================

TEST SUITE: 20 adversarial/edge case inputs designed to reveal bugs

RESULTS: 18/20 PASS (90%)

FAILURES:
1. All ASCII printable characters - Number sequence "0123456789" tokenized differently
2. Repeated numbers "1234567890" * 100 - Token count mismatch (334 vs 400)

ROOT CAUSE:
- rs-bpe implements STANDARD BPE CORRECTLY
- tiktoken uses REGEX PRE-TOKENIZATION that splits numbers into 1-3 digit groups
- Pattern: \p{N}{1,3}+ splits "7890" ‚Üí ["789", "0"] BEFORE applying BPE
- rs-bpe processes as continuous bytes ‚Üí "78" + "90" (correct by BPE rules)

EXAMPLE:
  Text: "7890"

  tiktoken:
    Pre-split: ["789", "0"]
    Tokens: [16474, 15]

  rs-bpe:
    No pre-split
    Applies merge '90' (rank 1954) before '789' (rank 16474)
    Tokens: [2495, 1954] ‚Üê CORRECT pure BPE

VERIFICATION:
  Manual BPE implementation confirms rs-bpe follows correct algorithm.
  Merge priorities: '90' (rank 1954) < '789' (rank 16474)
  Standard BPE applies lowest-rank merge first.

CODE ISSUE:
  Zig implementation stores pattern_str but doesn't use it:

  // tokenizer.zig:269
  pattern_str: []const u8,  // ‚úì Stored

  // tokenizer.zig:608
  pub fn encode(text) {
      return self.encodeHashMap(text);  // ‚úó Pattern not applied
  }

FIX NEEDED:
  Implement pre-tokenization using regex pattern before BPE encoding.
  This is required for strict tiktoken compatibility.

IMPACT:
  ‚úì Low priority - 90% of edge cases pass
  ‚úì Only affects number-heavy text
  ‚úì Most natural language works correctly
  ‚úó Required for exact GPT-4 tokenization compatibility

PASSING EDGE CASES:
‚úÖ Empty string
‚úÖ Single space
‚úÖ Single char
‚úÖ Simple word
‚úÖ Chinese text ("‰Ω†Â•Ω‰∏ñÁïå")
‚úÖ Emoji sequence ("üòÄüòÉüòÑüòÅ")
‚úÖ ZWJ emoji ("üë®‚Äçüë©‚Äçüëß‚Äçüë¶")
‚úÖ Very long repeated (1000 'a's)
‚úÖ Multiple newlines
‚úÖ Multiple spaces
‚úÖ Mixed whitespace
‚úÖ Special chars ("!@#$%^&*()")
‚úÖ Unicode combining (√© as e + combining acute)
‚úÖ Right-to-left (Arabic "ŸÖÿ±ÿ≠ÿ®ÿß ÿ®ŸÉ")
‚úÖ Null-like (\x00)
‚úÖ High Unicode (emoji via escape)
‚úÖ Mixed scripts ("Hello ‰∏ñÁïå ŸÖÿ±ÿ≠ÿ®ÿß")
‚úÖ Repeated phrase

FAILING EDGE CASES:
‚ùå All ASCII printable (contains "0123456789")
‚ùå Numbers only ("1234567890" * 100)

CONCLUSION:
rs-bpe is algorithmically CORRECT for pure BPE.
Differences from tiktoken are due to missing regex pre-tokenization feature.
Not a bug - a feature gap for strict GPT-4 compatibility.

FILES CREATED:
- test_edge_cases.py - Comprehensive test suite
- debug_failures.py - Detailed failure analysis
- verify_bpe_algorithm.py - Manual BPE verification
- investigate_tiktoken_pattern.py - Pattern behavior analysis
- EDGE_CASE_TEST_RESULTS.md - Full analysis document
- EDGE_CASE_SUMMARY.txt - This file

RUN TESTS:
  python3 test_edge_cases.py

===============================================================================
