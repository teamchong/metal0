[0;33mğŸ”¨ Building WASM tokenizer...[0m
[0;32mâœ… WASM build complete[0m

[0;34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
[0;32mğŸŒ Web/WASM Benchmark (hyperfine)[0m
[0;34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m

âš¡ Web/Node.js Benchmark: All 4 Libraries (realistic corpus)
============================================================
Encoding: 583 diverse texts (200K chars) Ã— 1000 iterations
Node.js startup overhead <2% with 1000 iterations

ğŸ“Š Benchmarking ALL 4 libraries:
   1. PyAOT (WASM)
   2. @anthropic-ai/tokenizer (JS)
   3. gpt-tokenizer (JS)
   4. tiktoken (Node)

Benchmark 1: PyAOT (WASM)
  Time (mean Â± Ïƒ):      50.6 ms Â±   1.4 ms    [User: 52.2 ms, System: 8.6 ms]
  Range (min â€¦ max):    48.8 ms â€¦  52.7 ms    5 runs
 
  Warning: Ignoring non-zero exit code.
 
Benchmark 2: @anthropic-ai/tokenizer (JS)
